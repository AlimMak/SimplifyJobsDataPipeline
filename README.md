# New Grad Job Listings - Apache Kafka Data Pipeline

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub README   â”‚â”€â”€â”€â–¶â”‚   Producer    â”‚â”€â”€â”€â–¶â”‚     Kafka Broker        â”‚
â”‚  (Data Source)   â”‚    â”‚  (Scraper +   â”‚    â”‚                         â”‚
â”‚                  â”‚    â”‚   Cleaner)    â”‚    â”‚  Topics:                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”œâ”€ raw-job-listings    â”‚
                                            â”‚  â”œâ”€ cleaned-jobs        â”‚
                                            â”‚  â”œâ”€ jobs-by-category    â”‚
                                            â”‚  â””â”€ job-alerts          â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚                         â”‚                    â”‚
                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                         â”‚ Consumer â”‚            â”‚  Consumer   â”‚    â”‚   Consumer   â”‚
                         â”‚ Group 1  â”‚            â”‚  Group 2    â”‚    â”‚   Group 3    â”‚
                         â”‚ (Store)  â”‚            â”‚ (Analytics) â”‚    â”‚  (Alerts)    â”‚
                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                         â”‚                    â”‚
                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                         â”‚  SQLite  â”‚            â”‚  Terminal   â”‚    â”‚  Alert Log   â”‚
                         â”‚    DB    â”‚            â”‚  Dashboard  â”‚    â”‚  (Filtered)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Kafka Concepts Demonstrated

| Concept          | Where It's Used                                                   |
|------------------|-------------------------------------------------------------------|
| **Producer**     | `producer.py` â€” Scrapes GitHub, cleans data, publishes to topics  |
| **Consumer**     | `consumer_store.py`, `consumer_analytics.py`, `consumer_alerts.py`|
| **Topics**       | `raw-job-listings`, `cleaned-jobs`, `jobs-by-category`, `job-alerts` |
| **Consumer Groups** | 3 independent groups processing the same data differently      |
| **Partitions**   | Jobs partitioned by category for parallel processing              |
| **Serialization**| JSON serialization/deserialization of job records                  |

## Project Structure

```
kafka-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml          # Kafka + Zookeeper infrastructure
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py               # Kafka configuration & topic definitions
â”‚   â”œâ”€â”€ scraper.py              # GitHub README HTML table parser & cleaner
â”‚   â”œâ”€â”€ models.py               # Job data models
â”‚   â”œâ”€â”€ producer.py             # Kafka producer - publishes job listings
â”‚   â”œâ”€â”€ consumer_store.py       # Consumer Group 1 - persists to SQLite
â”‚   â”œâ”€â”€ consumer_analytics.py   # Consumer Group 2 - real-time analytics
â”‚   â”œâ”€â”€ consumer_alerts.py      # Consumer Group 3 - filtered job alerts
â”‚   â”œâ”€â”€ stream_processor.py     # Kafka Streams-style processor (raw â†’ cleaned)
â”‚   â”œâ”€â”€ admin_topics.py         # Topic creation & management
â”‚   â””â”€â”€ run_pipeline.py         # Orchestrator to run the full pipeline
â””â”€â”€ data/
    â””â”€â”€ jobs.db                 # SQLite database (created at runtime)
```

## Quick Start

### 1. Start Kafka Infrastructure
```bash
docker-compose up -d
```

### 2. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 3. Create Kafka Topics
```bash
python src/admin_topics.py
```

### 4. Run the Full Pipeline
```bash
python src/run_pipeline.py
```

### Or Run Components Individually
```bash
# Terminal 1 - Start the stream processor (raw â†’ cleaned)
python src/stream_processor.py

# Terminal 2 - Start consumer group 1 (SQLite storage)
python src/consumer_store.py

# Terminal 3 - Start consumer group 2 (analytics dashboard)
python src/consumer_analytics.py

# Terminal 4 - Start consumer group 3 (job alerts)
python src/consumer_alerts.py

# Terminal 5 - Run the producer (scrape & publish)
python src/producer.py
```

## Data Cleaning Pipeline

The raw GitHub README contains HTML table rows with embedded links, emojis, and
formatting. The pipeline performs these cleaning steps:

1. **HTML Parsing** â€” Extracts table rows from the `<tbody>` elements
2. **Company Extraction** â€” Strips Simplify tracking links, extracts company name
3. **FAANG Detection** â€” Identifies ğŸ”¥ emoji markers for FAANG+ companies
4. **Sponsorship Flags** â€” Detects ğŸ›‚ (no sponsorship), ğŸ‡ºğŸ‡¸ (US citizenship), ğŸ”’ (closed), ğŸ“ (advanced degree)
5. **Location Normalization** â€” Handles multi-location `<details>` tags, `</br>` splits
6. **Application URL Extraction** â€” Pulls direct apply links from nested `<a>` tags
7. **Age Parsing** â€” Converts "0d", "1mo" etc. to days-ago integer
8. **Category Tagging** â€” Assigns jobs to categories based on section headers
9. **Deduplication** â€” Removes duplicate listings (same company + role + location)

## Configuration

Edit `src/config.py` to customize:

```python
KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"

# Alert filters
ALERT_LOCATIONS = ["Remote", "SF", "NYC", "Austin"]
ALERT_COMPANIES = ["Google", "Microsoft", "Amazon", "Meta"]
ALERT_MAX_AGE_DAYS = 7
```
